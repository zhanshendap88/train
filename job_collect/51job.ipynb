# -*- coding:UTF-8 -*-
#作者：hhs
#利用selenium自动化爬取前程无忧数据。仅供参考，内容设计略微简单

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import csv
import pandas as pd
import threading
from threading import Lock

def open_browser():
    options = Options()
    browser = webdriver.Chrome(options = options,executable_path=r'K:\sheng\anaconda\Scripts\chromedriver.exe')#路径需要更改
    return browser

def web_list():
    web_list =[]
    pages = [str(i) for i in range(1,2001)]
    for i in pages:
        front = 'https://search.51job.com/list/040000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590,2,'#网址需要调整
        back = '.html?lang=c&postchannel=0000&workyear=99&cotype=99&degreefrom=04&jobterm=99&companysize=99&ord_field=0&dibiaoid=0&line=&welfare='
        url = front + i + back
        web_list.append(url)         
    return web_list

def page_parse(browser,web_list,n,number,doucument_name):
    page_n = 2000//n
    count = 0
    for i in web_list[number * page_n: (number + 1) * page_n]:
        #捕获是否超时
        try:
            browser.get(i)
        except TimeoutException:
            print("Time Out.")
            print('中断的链接为{}'.format(i))
            break   
        
        job_list = browser.find_element_by_class_name('j_joblist')
        job_lists = job_list.find_elements_by_class_name('e')
        #页面内容检测
        if job_lists ==[]:
            print('第{}页已经开始没有内容，此线程爬虫结束'.format(count+(number * page_n)))
            break

        Dataset = []                       
        for j in job_lists:
            data = []                   
            try:
                job_herf = j.find_element_by_xpath('./a').get_attribute('href')
                data.append(job_herf)
            except NoSuchElementException:
                data.append("")
            
            try:
                job = j.find_element_by_xpath('./a/p[@class="t"]/span[1]').text
                data.append(job)
            except NoSuchElementException:
                data.append("")      
            
            try:
                Time = j.find_element_by_xpath('./a/p[@class="t"]/span[2]').text
                data.append(Time)
            except NoSuchElementException:
                data.append("")         
                
            try:
                job_salery = j.find_element_by_xpath('./a/p[@class="info"]/span[1]').text
                data.append(job_salery)
            except NoSuchElementException:
                data.append("")                  

            try:
                job_demand = j.find_element_by_xpath('./a/p[@class="info"]/span[2]').text
                data.append(job_demand)
            except NoSuchElementException:
                data.append("")  
                
            try:
                job_benefit = j.find_element_by_xpath('./a/p[@class="tags"]').get_attribute("title")
                data.append(job_benefit)
            except NoSuchElementException:
                data.append("")                  
                
            try:
                company = j.find_element_by_xpath('./div[@class="er"]/a')
                company_name = company.text
                company_href = company.get_attribute("href")
                data.append(company_name)
                data.append(company_href)
            except NoSuchElementException:
                data.append("")
                data.append("")
                               
            try:
                company_scale = j.find_element_by_xpath('./div[@class="er"]/p[1]').text
                data.append(company_scale)
            except NoSuchElementException:
                data.append("")                               
                               
            try:
                company_industry = j.find_element_by_xpath('./div[@class="er"]/p[2]').text
                data.append(company_industry)
            except NoSuchElementException:
                data.append("")
            
            Dataset.append(data)
        lock.acquire()
        save_data(Dataset,doucument_name)
        count +=1
        print("\r第{}页收集和储存完成".format(count+(number * page_n)))
        time.sleep(1)
        lock.release()
    browser.close()

def save_data(Dataset,doucument_name):
    with open(doucument_name,"a+",encoding="utf-8",newline='')as f:        
        file = csv.writer(f)
        
        try:
            df = pd.read_csv(doucument_name)            

        except:
            file.writerow(["jobhref","job","jobtime","salary","demand","benfit","company","companyherf","companyscale","companyindustry"])
            file.writerows(Dataset)
        
        else:
            file.writerows(Dataset)

def main():
    print("你想开设的线程数量")
    Thread_Count = int(input())
    print("你想保存的文件名")
    doucument_name = input()
    doucument_name =doucument_name+".csv"
    for number in range(Thread_Count):
        browser = open_browser()
        Web_List = web_list()
        var_name = "T" + str(number)
        locals()[var_name] = threading.Thread(target = page_parse,args= (browser,Web_List,Thread_Count,number,doucument_name,))
        locals()[var_name].start()

lock = Lock()
main()
